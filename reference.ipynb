{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sample run of the mini-sparkify dataset on a single node cluster on AWS-EMR with a Pyspark kernel. I will use this as a reference while training the large sparkify data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` __Installing and Importing Packages__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.1` Installing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52370c5d51284a708301e8b08551e425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>10</td><td>application_1619138142649_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-19-89.us-east-2.compute.internal:20888/proxy/application_1619138142649_0011/\" class=\"emr-proxy-link\" emr-resource=\"j-14BHGB2QI0DEB\n",
       "\" application-id=\"application_1619138142649_0011\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-19-89.us-east-2.compute.internal:8042/node/containerlogs/container_1619138142649_0011_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==0.25.1\n",
      "  Using cached https://files.pythonhosted.org/packages/7e/ab/ea76361f9d3e732e114adcd801d2820d5319c23d0ac5482fa3b412db217e/pandas-0.25.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==0.25.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==0.25.1)\n",
      "Collecting python-dateutil>=2.6.1 (from pandas==0.25.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-0.25.1 python-dateutil-2.8.1\n",
      "\n",
      "Collecting matplotlib\n",
      "  Using cached https://files.pythonhosted.org/packages/ce/63/74c0b6184b6b169b121bb72458818ee60a7d7c436d7b1907bd5874188c55/matplotlib-3.4.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib64/python3.7/site-packages (from matplotlib)\n",
      "Collecting pyparsing>=2.2.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/tmp/1619147428487-0/lib/python3.7/site-packages (from matplotlib)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/33/34/542152297dcc6c47a9dcb0685eac6d652d878ed3cea83bf2b23cb988e857/Pillow-8.2.0-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/46/231de802ade4225b76b96cffe419cf3ce52bbe92e3b092cf12db7d11c207/kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib)\n",
      "Installing collected packages: pyparsing, pillow, cycler, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.4.1 pillow-8.2.0 pyparsing-2.4.7"
     ]
    }
   ],
   "source": [
    "# Installing the packages pandas and matplotlib\n",
    "\n",
    "sc.install_pypi_package(\"pandas==0.25.1\")\n",
    "sc.install_pypi_package(\"matplotlib\", \"https://pypi.org/simple\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd59793c981d4c38b035270d0e494376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark2pmml\n",
      "  Downloading https://files.pythonhosted.org/packages/78/48/e3b2c8f52132eb1807d986f74b035cfc6114f42a0310d6d905b90f63a3cc/pyspark2pmml-0.5.1.tar.gz\n",
      "Collecting py4j (from pyspark2pmml)\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/e2/543019a6e620b759a59f134158b4595766f9bf520a1081a2ba1a1809ba32/py4j-0.10.9.2-py2.py3-none-any.whl (198kB)\n",
      "Building wheels for collected packages: pyspark2pmml\n",
      "  Running setup.py bdist_wheel for pyspark2pmml: started\n",
      "  Running setup.py bdist_wheel for pyspark2pmml: finished with status 'done'\n",
      "  Stored in directory: /var/lib/livy/.cache/pip/wheels/fa/f2/e9/e2370733daa2c5fc3271a64eba149a57d44a607901760d17b3\n",
      "Successfully built pyspark2pmml\n",
      "Installing collected packages: py4j, pyspark2pmml\n",
      "Successfully installed py4j-0.10.9.2 pyspark2pmml-0.5.1"
     ]
    }
   ],
   "source": [
    "# pyspark2pmml may help in saving a pyspark model as a pmml file\n",
    "sc.install_pypi_package(\"pyspark2pmml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.2` Importing general libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedf90a64ad9484d87e661f48e514fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.3` Importing pyspark based libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095457ea2a884826aea72ae6b402935d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import libraries for spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, DateType, IntegerType\n",
    "from pyspark.sql.functions import concat, lit, avg, split, isnan, when, count, col, sum, mean, stddev, min, max, round, udf, to_date, datediff \n",
    "from pyspark.sql import Window\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, Normalizer, StandardScaler, MinMaxScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, NaiveBayes, RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4e24a795c449469fdda2a7d7b40d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version  \n",
      "-------------------------- ---------\n",
      "beautifulsoup4             4.9.3    \n",
      "boto                       2.49.0   \n",
      "click                      7.1.2    \n",
      "cycler                     0.10.0   \n",
      "jmespath                   0.10.0   \n",
      "joblib                     1.0.1    \n",
      "kiwisolver                 1.3.1    \n",
      "lxml                       4.6.2    \n",
      "matplotlib                 3.4.1    \n",
      "mysqlclient                1.4.2    \n",
      "nltk                       3.5      \n",
      "nose                       1.3.4    \n",
      "numpy                      1.16.5   \n",
      "pandas                     0.25.1   \n",
      "Pillow                     8.2.0    \n",
      "pip                        9.0.1    \n",
      "py-dateutil                2.2      \n",
      "pyparsing                  2.4.7    \n",
      "python-dateutil            2.8.1    \n",
      "python37-sagemaker-pyspark 1.4.1    \n",
      "pytz                       2021.1   \n",
      "PyYAML                     5.4.1    \n",
      "regex                      2021.3.17\n",
      "setuptools                 28.8.0   \n",
      "six                        1.13.0   \n",
      "tqdm                       4.59.0   \n",
      "wheel                      0.29.0   \n",
      "windmill                   1.6"
     ]
    }
   ],
   "source": [
    "# List the current packages available\n",
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` __Setting up a spark session__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea57e2540ff425e88177348b1ea2c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"spark_app\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'proxyUser': 'user_motua16', 'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>10</td><td>application_1619138142649_0011</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-19-89.us-east-2.compute.internal:20888/proxy/application_1619138142649_0011/\" class=\"emr-proxy-link\" emr-resource=\"j-14BHGB2QI0DEB\n",
       "\" application-id=\"application_1619138142649_0011\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-19-89.us-east-2.compute.internal:8042/node/containerlogs/container_1619138142649_0011_01_000001/livy\" >Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# getting information about the current session configuration\n",
    "\n",
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv', 'driverMemory': '6000M'}, 'proxyUser': 'user_motua16', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# configuring the session\n",
    "\n",
    "%%configure -f \n",
    "\n",
    "{ \"conf\":{\n",
    "          \"spark.pyspark.python\": \"python3\", \"spark.pyspark.virtualenv.enabled\": \"true\", \"spark.pyspark.virtualenv.type\": \"native\", \"spark.pyspark.virtualenv.bin.path\": \"/usr/bin/virtualenv\", \"driverMemory\": \"6000M\"\n",
    "         }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3` __Feature Engineering__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.1` Extracting useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e694873011ec4c21a1322f5f1957d0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def clean_data(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This functions removes all the rows where userId is empty and \n",
    "    returns the dataframe.\n",
    "    \n",
    "    Input : DataFrame\n",
    "    Output : cleaned DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df_new = df.filter(df[\"userId\"] != \"\")\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "def prepare_dataset(df): \n",
    "    \n",
    "    \"\"\"\n",
    "    This function will prepare the DataFrame for Machine Learning\n",
    "    by Extracting out the useful features, and engineering more \n",
    "    relevant features. The unique users will be kept in one of the DataFrames\n",
    "    which will be used for ML\n",
    "    \n",
    "    Input : DataFrame\n",
    "    Output : DataFrame to be used for applying Machine Learning and the modified input DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    df = clean_data(df)\n",
    "    \n",
    "    \n",
    "    \"\"\"Defining churn\"\"\"\n",
    "    cancellation_event = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())   \n",
    "    df = df.withColumn(\"churn\", cancellation_event(\"page\"))   \n",
    "    cancelled_users = df.select(['userId']).where(df.churn == 1).groupby('userId').count().toPandas()['userId'].values\n",
    "    \n",
    "    #Filling all the users who pressed the 'Cancellation Confirmation' button with 1\n",
    "    def fill_array(userId, features):\n",
    "        if(userId in cancelled_users): return 1\n",
    "        else : return 0\n",
    "        \n",
    "    fill_array_udf = udf(fill_array, IntegerType())\n",
    "    df = df.withColumn(\"churn\", fill_array_udf(col(\"userId\"), col(\"churn\")))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    w = Window.partitionBy('userId') #Partitioning the Data by User Id\n",
    "    df = df.withColumn('last_ts', max('ts').over(w)) #create last timestamp\n",
    "    df = df.withColumn('first_ts', min('ts').over(w)) #create first timestamp\n",
    "    \n",
    "    #This function will convert timestamp to date\n",
    "    def get_date_from_ts(ts):\n",
    "        return str(datetime.utcfromtimestamp(ts / 1000).strftime('%Y-%m-%d'))\n",
    "    \n",
    "    get_date_from_ts_udf = udf(get_date_from_ts, StringType())\n",
    "    df = df.withColumn('last_date', get_date_from_ts_udf(col('last_ts'))) #converting last timestamp to date\n",
    "    df = df.withColumn('first_date', get_date_from_ts_udf(col('first_ts'))) #converting first timestamp to date\n",
    "    \n",
    "    \n",
    "    df = df.withColumn('date', get_date_from_ts_udf(col('ts'))) #converting all timestamps to date\n",
    "    \n",
    "    df = df.withColumn('last_level',when(df.last_ts == df.ts, df.level)) #defining the last level (paid or free) of a user\n",
    "    \n",
    "    # create column avg_songs to calculate average number of songs per day\n",
    "    # first grouping on unique (userId, date) pair and then taking average \n",
    "    # over all the dates for a particular user\n",
    "    w = Window.partitionBy('userId', 'date')\n",
    "    songs = df.where(df.page == 'NextSong').select('userId', 'date', count('userId').over(w).alias('songs')).distinct()\n",
    "    w = Window.partitionBy('userId')\n",
    "    songs = songs.withColumn('avg_songs', avg('songs').over(w))\n",
    "    songs = songs.select(col(\"userId\").alias(\"songs_userId\"), 'avg_songs')\n",
    "    songs = songs.withColumn(\"avg_songs\", round(songs[\"avg_songs\"], 2))\n",
    "    \n",
    "    # create column avg_events to calculate average number of events per day\n",
    "    # first grouping on unique (userId, date) pair and then taking average \n",
    "    # over all the dates for a particular user\n",
    "    w = Window.partitionBy('userId', 'date')\n",
    "    events = df.select('userId', 'date', count('userId').over(w).alias('events')).distinct()\n",
    "    w = Window.partitionBy('userId')\n",
    "    events = events.withColumn('avg_events', avg('events').over(w))\n",
    "    events = events.select(col(\"userId\").alias(\"events_userId\"), 'avg_events')\n",
    "    events = events.withColumn(\"avg_events\", round(events[\"avg_events\"], 2))\n",
    "    \n",
    "    # calculate number of thumbs up for a user\n",
    "    w = Window.partitionBy('userId')\n",
    "    thumbsup = df.where(df.page == 'Thumbs Up').select('userId', count('userId').over(w).alias('thumbs_up')).distinct()\n",
    "    thumbsup = thumbsup.select(col(\"userId\").alias(\"thumbsup_userId\"), 'thumbs_up')\n",
    "    \n",
    "    # calculate number of thumbs down for a user\n",
    "    w = Window.partitionBy('userId')\n",
    "    thumbsdown = df.where(df.page == 'Thumbs Down').select('userId', count('userId').over(w).alias('thumbs_down')).distinct()\n",
    "    thumbsdown = thumbsdown.select(col(\"userId\").alias(\"thumbsdown_userId\"), 'thumbs_down')\n",
    "    \n",
    "    # calculate days since the date of the first event\n",
    "    df = df.withColumn(\"days_active\", \n",
    "              datediff(to_date(lit(datetime.now().strftime(\"%Y-%m-%d %H:%M\"))),\n",
    "                       to_date(\"first_date\",\"yyyy-MM-dd\")))\n",
    "    \n",
    "    # add column with state of the event based on location column\n",
    "    def get_state(location):\n",
    "        location = location.split(',')[-1].strip()\n",
    "        if (len(location) > 2):\n",
    "            location = location.split('-')[-1].strip()\n",
    "    \n",
    "        return location\n",
    "    \n",
    "    get_state_udf = udf(get_state, StringType())\n",
    "    df = df.withColumn('state', get_state_udf(col('location')))\n",
    "    \n",
    "    #add column with last location of the user\n",
    "    df = df.withColumn('last_state',when(df.last_ts == df.ts, df.state))\n",
    "    \n",
    "    # calculate number of add friends for a user\n",
    "    w = Window.partitionBy('userId')\n",
    "    addfriend = df.where(df.page == 'Add Friend').select('userId', count('userId').over(w).alias('addfriend')).distinct()\n",
    "    addfriend = addfriend.select(col(\"userId\").alias(\"addfriend_userId\"), 'addfriend')\n",
    "\n",
    "    # assemble everything into resulting dataset\n",
    "    df_ml = df.select('userId', 'gender', 'churn', 'last_level', 'days_active', 'last_state')\\\n",
    "    .dropna().drop_duplicates()\n",
    "    df_ml = df_ml.join(songs, df_ml.userId == songs.songs_userId).distinct()\n",
    "    df_ml = df_ml.join(events, df_ml.userId == events.events_userId).distinct()\n",
    "    df_ml = df_ml.join(thumbsup, df_ml.userId == thumbsup.thumbsup_userId, how='left').distinct()\n",
    "    df_ml = df_ml.fillna(0, subset=['thumbs_up'])\n",
    "    df_ml = df_ml.join(thumbsdown, df_ml.userId == thumbsdown.thumbsdown_userId, how='left').distinct()\n",
    "    df_ml = df_ml.fillna(0, subset=['thumbs_down'])\n",
    "    df_ml = df_ml.join(addfriend, df_ml.userId == addfriend.addfriend_userId, how='left').distinct()\n",
    "    df_ml = df_ml.fillna(0, subset=['addfriend'])\n",
    "    df_ml = df_ml.drop('songs_userId','events_userId', 'thumbsup_userId', 'thumbsdown_userId', 'addfriend_userId')\n",
    "    \n",
    "    return df, df_ml\n",
    "    \n",
    "df = spark.read.json('s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json')\n",
    "df.persist()\n",
    "\n",
    "df, df_ml = prepare_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.2` Indexing of String columns and One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8aa1103cffb49f9b453630d645e6780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# index categorical features gender, level and state (like Label Encoder in sklearn)\n",
    "\n",
    "stringIndexerGender = StringIndexer(inputCol=\"gender\", outputCol=\"genderIndex\", handleInvalid = 'skip')\n",
    "stringIndexerLevel = StringIndexer(inputCol=\"last_level\", outputCol=\"levelIndex\", handleInvalid = 'skip')\n",
    "stringIndexerState = StringIndexer(inputCol=\"last_state\", outputCol=\"stateIndex\", handleInvalid = 'skip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28a8305655d3435183691090cb394760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OneHotEncoding these features\n",
    "\n",
    "encoder_gender = OneHotEncoder(inputCol=\"genderIndex\", outputCol=\"genderVec\")\n",
    "encoder_level = OneHotEncoder(inputCol=\"levelIndex\", outputCol=\"levelVec\")\n",
    "encoder_state = OneHotEncoder(inputCol=\"stateIndex\", outputCol=\"stateVec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4` __Applying Machine Learning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf2b33900914d50917f95ebb5996145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create vector for features\n",
    "features = ['genderVec', 'levelVec', 'stateVec', 'days_active', 'avg_songs', 'avg_events', 'thumbs_up', 'thumbs_down', 'addfriend']\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# initialize random forest classifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# assemble pipeline\n",
    "pipeline = Pipeline(stages = [stringIndexerGender, stringIndexerLevel, stringIndexerState, encoder_gender,encoder_level,encoder_state, assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db81798dbc6e451880a5b726e3f05d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train-test-validation split\n",
    "\n",
    "df_ml = df_ml.withColumnRenamed(\"churn\", \"label\")\n",
    "\n",
    "train, test_valid = df_ml.randomSplit([0.6, 0.4], seed = 42)\n",
    "test, validation = test_valid.randomSplit([0.5, 0.5], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efb0859f6534221bb854d0c2a5eb8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fitting the model\n",
    "\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a6e525407c4934af1b027134bc790b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# saving the model in a S3 bucket\n",
    "model.save('s3://aws-emr-resources-816555935147-us-east-2/notebooks/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('s3://bucket-motua16/model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
